{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4040\n"
     ]
    }
   ],
   "source": [
    "f = open(\"train.json\", \"r\")\n",
    "count = 0\n",
    "labels = []\n",
    "text = []\n",
    "num_data = []\n",
    "\n",
    "try:\n",
    "    data = json.load(f)\n",
    "    \n",
    "    for item in data:\n",
    "        \n",
    "        # extract the outcome labels and text for request and title\n",
    "        labels.append(int(item['requester_received_pizza']))\n",
    "        #text.append(item['request_text'] + ' ' + item['request_title'])\n",
    "        text.append(item['request_text'])\n",
    "        \n",
    "        # extract numeric data\n",
    "        num = []\n",
    "        num.append(item['number_of_downvotes_of_request_at_retrieval'])\n",
    "        num.append(item['number_of_upvotes_of_request_at_retrieval'])\n",
    "        num.append(item['request_number_of_comments_at_retrieval'])\n",
    "        num.append(item['requester_account_age_in_days_at_request'])\n",
    "        num.append(item['requester_days_since_first_post_on_raop_at_request'])\n",
    "        num.append(item['requester_number_of_comments_at_retrieval'])\n",
    "        num.append(item['requester_number_of_comments_in_raop_at_retrieval'])\n",
    "        num.append(item['requester_number_of_posts_at_request'])\n",
    "        num.append(item['requester_number_of_subreddits_at_request'])\n",
    "        num.append(len(item['requester_subreddits_at_request']))\n",
    "        num.append(item['unix_timestamp_of_request_utc'])\n",
    "        num.append(len(item['request_text']))\n",
    "        num.append(len(item['request_title']))\n",
    "        num_data.append(num)\n",
    "\n",
    "except Exception, e:\n",
    "    print \"error reading from file: %s\" %e\n",
    "    f.close()\n",
    "\n",
    "f.close()\n",
    "print len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = np.array(labels[:3500])\n",
    "train_data = np.array(text[:3500])\n",
    "train_nums = np.array(num_data[:3500])\n",
    "\n",
    "dev_labels = np.array(labels[3500:])\n",
    "dev_data = np.array(text[3500:])\n",
    "dev_nums = np.array(num_data[3500:])\n",
    "\n",
    "# normalize the numerical data\n",
    "for i in range(train_nums.shape[1]):\n",
    "    max1 = np.mean(train_nums[:,i])\n",
    "    max2 = np.mean(dev_nums[:,i])\n",
    "    \n",
    "    train_nums[:,i] = train_nums[:,i]/max1\n",
    "    dev_nums[:,i] = dev_nums[:,i]/max2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://m.youtube.com/?reason=8&amp;rdm=3676#/watch?v=V09Mt66O9_c&amp;desktop_uri=%2Fwatch%3Fv%3DV09Mt66O9_c [Request] Columbus OH I am ready to play! 0\n",
      "(540, 13) (3500, 13)\n",
      "80.0\n"
     ]
    }
   ],
   "source": [
    "print dev_data[150], dev_labels[150]\n",
    "\n",
    "print dev_nums.shape, train_nums.shape\n",
    "print np.max(dev_nums[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3500, 107742) (540, 107742)\n",
      "base f1 = 0.290155440415\n",
      "friend\n",
      "love pie\n",
      "could make\n",
      "when have\n",
      "food for\n",
      "Improved f1 = 0.27358490566\n",
      "draft\n",
      "aid\n",
      "surpr\n",
      "battle\n",
      "NEED\n"
     ]
    }
   ],
   "source": [
    "def better_preprocessor(s):\n",
    "\n",
    "    # remove links and replace with string \"hyperlink\"\n",
    "    links = re.compile(u'\\\\bhttp\\w+\\\\b')\n",
    "    # remove non-alphanumerics\n",
    "    non_alpha = re.compile(u'[\\W_]+', re.UNICODE)\n",
    "    # change all number sequences to single '0'\n",
    "    num = re.compile(u'[0-9]+', re.UNICODE)\n",
    "    # list of common English suffixes to remove\n",
    "    suff = re.compile(u'(al|ance|ence|dom|ed|er|or|ism|ist|ity|ty|ment|ship|sion|ing|s|ly|ation|tion|able|ible|ate|en|ify|fy|ize|ise|ful|ic|ical|ious|ous|ish|ive)\\\\b', re.UNICODE)\n",
    "    # list of common English prefixes to remove\n",
    "    pre = re.compile(u'\\\\b(an|ante|anti|auto|circum|co|com|con|contra|de|dis|en|ex|extra|hyper|in|im|il|ir|inter|intra|macro|micro|mid|mis|mono|non|over|post|pre|pro|re|semi|sub|super|trans|tri|un)', re.UNICODE)\n",
    "    # remove words with one or two characters only (treat as stopwords)\n",
    "    stop = re.compile(u'\\\\b([\\w]{1,2})\\\\b', re.UNICODE)\n",
    "\n",
    "    # apply filters\n",
    "    t = links.sub(u'hyperlink', s)\n",
    "    t = non_alpha.sub(u' ', t)\n",
    "    t = num.sub(u\" 0 \", t)\n",
    "    t = suff.sub(u' ', t)\n",
    "    t = pre.sub(u' ', t)\n",
    "    t = stop.sub(u' ', t)\n",
    "    return t\n",
    "\n",
    "# use vectorize text data and then train logistic regression\n",
    "def trainLM():\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", ngram_range=[1,2])\n",
    "    X = vectorizer.fit_transform(train_data)\n",
    "    Y = vectorizer.transform(dev_data)\n",
    "    vocab = vectorizer.get_feature_names()\n",
    "    \n",
    "    print X.shape, Y.shape\n",
    "    \n",
    "    lm = LogisticRegression()\n",
    "    lm.fit(X, train_labels)\n",
    "    pred_labels = lm.predict(Y)\n",
    "    \n",
    "    score = metrics.f1_score(dev_labels, pred_labels)\n",
    "    print \"base f1 =\", score\n",
    "    \n",
    "    for row in range(lm.coef_.shape[0]):\n",
    "        maxind = np.fabs(lm.coef_[row]).argsort()[-5:]\n",
    "        for item in maxind:\n",
    "            print vocab[item]\n",
    "    \n",
    "    # vectorize data using the better preprocessor\n",
    "    better_vect = CountVectorizer(preprocessor=better_preprocessor)\n",
    "    better_X = better_vect.fit_transform(train_data)\n",
    "    better_Y = better_vect.transform(dev_data)\n",
    "    vocab2 = better_vect.get_feature_names()\n",
    "    \n",
    "    # run logistic regression on this data and report f1 score\n",
    "    better_lm = LogisticRegression()\n",
    "    better_lm.fit(better_X, train_labels)\n",
    "    better_labels = better_lm.predict(better_Y)\n",
    "    print \"Improved f1 =\", metrics.f1_score(dev_labels, better_labels)\n",
    "    \n",
    "    for row in range(better_lm.coef_.shape[0]):\n",
    "        #maxind2 = np.fabs(better_lm.coef_[row]).argsort()[-5:]\n",
    "        maxind2 = better_lm.coef_[row].argsort()[-5:]\n",
    "        for item in maxind2:\n",
    "            print vocab2[item]\n",
    "    \n",
    "trainLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.767\n",
      "accuracy for BernoulliNB is 0.757\n"
     ]
    }
   ],
   "source": [
    "# train a simple KNN model and check accuracy\n",
    "# this uses the numerical data from json input\n",
    "def trainKNN():\n",
    "    # value for k from gridsearch output\n",
    "    model = KNeighborsClassifier(n_neighbors=47)\n",
    "    model.fit(train_nums, train_labels)\n",
    "    pred_labels = model.predict(dev_nums)\n",
    "    \n",
    "    correct = (pred_labels == dev_labels)\n",
    "    accuracy = 1.0 * np.sum(correct) / len(pred_labels)\n",
    "    print \"%.03f\" %accuracy\n",
    "\n",
    "# run gridsearch to find best k value\n",
    "def findK():\n",
    "    # range is from an iterative process to narrow down\n",
    "    params = {'n_neighbors': range(38,56,1)}\n",
    "    search = GridSearchCV(KNeighborsClassifier(), params)\n",
    "    search.fit(train_nums, train_labels)\n",
    "    \n",
    "    print \"the best parameter is k=%f\\n\" %search.best_params_['n_neighbors']\n",
    "    print \"summary of all params:\\n\", search.grid_scores_\n",
    "\n",
    "# train a simple BernoulliNB model and check accuracy\n",
    "# this uses the numeric data captured from json input\n",
    "def trainBern():\n",
    "    bern = BernoulliNB(alpha=0)\n",
    "    bern.fit(train_nums, train_labels)\n",
    "    pred_labels = bern.predict(dev_nums)\n",
    "    \n",
    "    # display the accuracy\n",
    "    correct = (pred_labels == dev_labels)\n",
    "    accuracy = 1.0 * np.sum(correct) / len(dev_labels)\n",
    "    print \"accuracy for BernoulliNB is %.03f\" %accuracy\n",
    "\n",
    "# run gridsearch to find parameters for NB model\n",
    "def bernParams():\n",
    "    params = {'alpha': [0.0, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0]}\n",
    "    search = GridSearchCV(BernoulliNB(), params)\n",
    "    search.fit(train_nums, train_labels)\n",
    "    \n",
    "    print \"the best parameter is k=%f\\n\" %search.best_params_['alpha']\n",
    "    print \"summary of all params:\\n\", search.grid_scores_\n",
    "\n",
    "trainKNN()\n",
    "#findK()\n",
    "trainBern()\n",
    "#bernParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
