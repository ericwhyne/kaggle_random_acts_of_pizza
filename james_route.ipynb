{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "import json\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# import nltk functions for manipulating text. textblob for sentiment analysis\n",
    "import nltk\n",
    "from nltk.tokenize.regexp import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4040\n",
      "(3500, 36)\n"
     ]
    }
   ],
   "source": [
    "# keywords to extract as features\n",
    "highprob = [u'kindle', u'randomacts', u'kindle', u'randomacts', u'surviving', u'stretch', u'electronics', u'electronics', u'stopped', u'heat', u'total', u'checks', u'landlord', u'cover', u'RandomActsOfChristmas', u'RandomActsOfChristmas']\n",
    "\n",
    "# stopwords to filter with nltk\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "# tokenize using this regex\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "\n",
    "# filter hyperlinks with this regex\n",
    "links = re.compile(r'\\bhttp\\S+\\b')\n",
    "\n",
    "# lemmatize the tokens\n",
    "wl = WordNetLemmatizer()\n",
    "\n",
    "# lists for storing labels, request text, metadata for each request, and request ids\n",
    "labels = []\n",
    "text = []\n",
    "num_data = []\n",
    "ids = []\n",
    "count = 0\n",
    "\n",
    "# open this file for reading\n",
    "f = open(\"train.json\", \"r\")\n",
    "\n",
    "try:\n",
    "    data = json.load(f)\n",
    "    \n",
    "    for item in data:\n",
    "        \n",
    "        # extract the outcome labels\n",
    "        labels.append(int(item['requester_received_pizza']))\n",
    "        \n",
    "        # keep track of the request ids. useful for debugging\n",
    "        ids.append(item['request_id'])\n",
    "        \n",
    "        # get the text of request\n",
    "        words = unicode(item['request_text_edit_aware'])\n",
    "        \n",
    "        # remove links and replace with string \"hyperlink.\" this prevents lots of spurious tokens\n",
    "        words = links.sub(u'hyperlink', words)\n",
    "        \n",
    "        # tokenize and lemmatize\n",
    "        tokens = [t for t in tokenizer.tokenize(words) if t.lower() not in sw]\n",
    "        tokens = [wl.lemmatize(t.lower()) for t in tokens]\n",
    "\n",
    "        # convert tokens back to string and store it\n",
    "        text.append(' '.join(tokens))\n",
    "        \n",
    "        # get the title of request, tokenize, stem/lemmatize\n",
    "        title_words = unicode(item['request_title'])\n",
    "        title_tokens = [t for t in tokenizer.tokenize(title_words) if t.lower() not in sw]\n",
    "        title_tokens = [wl.lemmatize(t.lower()) for t in title_tokens]\n",
    "        \n",
    "        # combine tokens from text and title\n",
    "        tokens += title_tokens\n",
    "        \n",
    "        # extract metadata. store data for each request in num\n",
    "        num = []\n",
    "        num.append(item['requester_account_age_in_days_at_request']/100.0)\n",
    "        num.append(item['requester_days_since_first_post_on_raop_at_request'])\n",
    "        num.append(item['requester_number_of_comments_at_request']/100.0)\n",
    "        num.append(int(item['requester_number_of_comments_in_raop_at_request'] > 0))\n",
    "        num.append(item['requester_number_of_posts_at_request'])\n",
    "        num.append(item['requester_number_of_subreddits_at_request'])\n",
    "        num.append(int(item['requester_number_of_posts_on_raop_at_request'] > 0))\n",
    "        num.append(item['requester_upvotes_minus_downvotes_at_request'])\n",
    "        num.append(item['requester_upvotes_plus_downvotes_at_request'])\n",
    "        \n",
    "        # store the number of tokens in the request and title as additional features\n",
    "        num.append(len(tokens))\n",
    "        num.append(len(title_tokens))\n",
    "        \n",
    "        # extract one feature for each word in the keyword array\n",
    "        for word in highprob:\n",
    "            num.append(int(word in tokens))\n",
    "        \n",
    "        # manually check for jpg in the request. ugly, but we already filtered urls\n",
    "        if 'jpg' in item['request_text_edit_aware']:\n",
    "            num.append(1)\n",
    "        else:\n",
    "            num.append(0)\n",
    "        \n",
    "        #num.append(int('hyperlink' in tokens))\n",
    "       \n",
    "        # get the day of the month request occurred. set feature to 1 if in first half of month\n",
    "        # additional experimentation shows that this is the best day to select\n",
    "        dt = int(datetime.datetime.fromtimestamp(item['unix_timestamp_of_request_utc']).strftime('%d'))\n",
    "        num.append(int(dt <= 15))\n",
    "            \n",
    "        # append the entire list to num_data, which stores data for all examples\n",
    "        num_data.append(num)\n",
    "        count += 1\n",
    "\n",
    "except Exception, e:\n",
    "    print \"error reading from file: %s\" %e\n",
    "    f.close()\n",
    "\n",
    "f.close()\n",
    "print len(labels)\n",
    "\n",
    "# convert python lists to numpy arrays\n",
    "# _data contains the text data, and _nums contains the metadata\n",
    "train_labels = np.array(labels[:3500])\n",
    "train_data = np.array(text[:3500])\n",
    "train_nums = np.array(num_data[:3500])\n",
    "print train_nums.shape\n",
    "\n",
    "dev_labels = np.array(labels[3500:])\n",
    "dev_data = np.array(text[3500:])\n",
    "dev_nums = np.array(num_data[3500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.757\n",
      "false +, false - : 0, 131\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "accuracy for BernoulliNB is 0.733\n",
      "accuracy for LM is 0.761\n",
      "false +, false - : 6, 123\n",
      "[u't3_uvoyu' u't3_1exnem' u't3_lcs2n' u't3_q21uq' u't3_132u9h' u't3_w4fdd'\n",
      " u't3_i3ms5' u't3_12p0tt' u't3_zpn24' u't3_1mjbbd' u't3_lcqaj' u't3_ibua5'\n",
      " u't3_qnocf' u't3_w5491' u't3_1ik128' u't3_w8mfc' u't3_ijcon' u't3_u69ow'\n",
      " u't3_hutzt' u't3_1chlic' u't3_ur5yf' u't3_z7mrn' u't3_1ihhx4' u't3_j8fnb'\n",
      " u't3_hucws' u't3_ojq68' u't3_12zfh3' u't3_sp8xg' u't3_pk4g1' u't3_ktlwy'\n",
      " u't3_1lojow' u't3_1dr9hu' u't3_j94no' u't3_ihorg' u't3_y9vxk' u't3_11lvl7'\n",
      " u't3_1l5cvv' u't3_qqotp' u't3_j6433' u't3_icpji' u't3_l9g0n' u't3_ikszl'\n",
      " u't3_1enrol' u't3_idifo' u't3_xtx62' u't3_1ifnr4' u't3_ilau5' u't3_uotiq'\n",
      " u't3_1jlnc4' u't3_iekd3' u't3_19gv1n' u't3_o8aso' u't3_o4fyx' u't3_m7to4'\n",
      " u't3_wgk03' u't3_18zn0d' u't3_xeser' u't3_17vccr' u't3_kfxsm' u't3_1j485g'\n",
      " u't3_kwqew' u't3_ibvsg' u't3_1f530o' u't3_1jgj2w' u't3_js9be' u't3_ll3uk'\n",
      " u't3_jtkzo' u't3_182nt8' u't3_iqm6n' u't3_l7opc' u't3_tpezd' u't3_xd34k'\n",
      " u't3_1ich4u' u't3_152l21' u't3_rowqk' u't3_1bkedk' u't3_17z4w1'\n",
      " u't3_15uyq3' u't3_huzao' u't3_jp7f2' u't3_14sy0u' u't3_n955o' u't3_w7yx5'\n",
      " u't3_18920c' u't3_ig6ya' u't3_ide15' u't3_j0st7' u't3_v8h8a' u't3_183m94'\n",
      " u't3_1nphr4' u't3_u027s' u't3_kj1lv' u't3_198a81' u't3_nztz2' u't3_oqt9r'\n",
      " u't3_jxdjg' u't3_1czlly' u't3_nucpj' u't3_1d4ncf' u't3_jd4dp' u't3_uvavy'\n",
      " u't3_m6ue5' u't3_19t34q' u't3_105fpq' u't3_16y5eo' u't3_ihosh' u't3_ionv5'\n",
      " u't3_1mwwu5' u't3_rn074' u't3_11zb19' u't3_hj1hr' u't3_15ybss'\n",
      " u't3_1fw7rg' u't3_1dnhcy' u't3_1eilsp' u't3_wyilc' u't3_uiv13' u't3_i5c8j'\n",
      " u't3_ibfwq' u't3_j06yw' u't3_penyh' u't3_so8v3' u't3_oo525']\n"
     ]
    }
   ],
   "source": [
    "# train a simple KNN model and check accuracy\n",
    "# this uses the metadata\n",
    "def trainKNN(k=1):\n",
    "    # value for k from gridsearch output\n",
    "    model = KNeighborsClassifier(n_neighbors=38)\n",
    "    model.fit(train_nums, train_labels)\n",
    "    pred_labels = model.predict(dev_nums)\n",
    "    \n",
    "    # calculate accuracy and identify the false positive and negative cases\n",
    "    correct = (pred_labels == dev_labels)\n",
    "    false_pos = (pred_labels > dev_labels)\n",
    "    false_neg = (pred_labels < dev_labels)\n",
    "    accuracy = 1.0 * np.sum(correct) / len(pred_labels)\n",
    "    print \"%.03f\" %accuracy\n",
    "    print \"false +, false - : %d, %d\" %(false_pos.sum(), false_neg.sum())\n",
    "    \n",
    "    # get the request ids for the dev set and print ids for false pos or neg cases\n",
    "    np_ids = np.array(ids[3500:])\n",
    "    print np_ids[false_pos]\n",
    "    print pred_labels[false_pos]\n",
    "    print dev_labels[false_pos]\n",
    "\n",
    "# run gridsearch to find best k value\n",
    "def findK():\n",
    "    # range is from an iterative process to narrow down\n",
    "    params = {'n_neighbors': range(38,56,1)}\n",
    "    search = GridSearchCV(KNeighborsClassifier(), params)\n",
    "    search.fit(train_nums, train_labels)\n",
    "    \n",
    "    print \"the best parameter is k=%f\\n\" %search.best_params_['n_neighbors']\n",
    "    print \"summary of all params:\\n\", search.grid_scores_\n",
    "\n",
    "# train a simple BernoulliNB model and check accuracy\n",
    "# this uses the metadata\n",
    "def trainBern():\n",
    "    bern = BernoulliNB(alpha=20.0, binarize=0.9)\n",
    "    bern.fit(train_nums, train_labels)\n",
    "    pred_labels = bern.predict(dev_nums)\n",
    "    \n",
    "    # display the accuracy\n",
    "    correct = (pred_labels == dev_labels)\n",
    "    accuracy = 1.0 * np.sum(correct) / len(dev_labels)\n",
    "    print \"accuracy for BernoulliNB is %.03f\" %accuracy\n",
    "\n",
    "# run gridsearch to find parameters for NB model\n",
    "def bernParams():\n",
    "    #params = {'binarize': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}\n",
    "    #params = {'alpha': [0.0, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0, 20.0, 50.0]}\n",
    "    search = GridSearchCV(BernoulliNB(binarize=0.9), params)\n",
    "    search.fit(train_nums, train_labels)\n",
    "    \n",
    "    print \"the best parameter is alpha=%f\\n\" %search.best_params_['alpha']\n",
    "    print \"summary of all params:\\n\", search.grid_scores_\n",
    "\n",
    "# train logistic regression using the metadata\n",
    "def trainNumLM():\n",
    "    lm = LogisticRegression()\n",
    "    lm.fit(train_nums, train_labels)\n",
    "    pred_labels = lm.predict(dev_nums)\n",
    "    \n",
    "    # print accuracy\n",
    "    correct = (pred_labels == dev_labels)\n",
    "    accuracy = 1.0 * np.sum(correct) / len(dev_labels)\n",
    "    print \"accuracy for LM is %.03f\" %accuracy\n",
    "    \n",
    "    # get false positive and negative cases and print data for the cases\n",
    "    false_pos = (pred_labels > dev_labels)\n",
    "    false_neg = (pred_labels < dev_labels)\n",
    "    print \"false +, false - : %d, %d\" %(false_pos.sum(), false_neg.sum())\n",
    "    np_ids = np.array(ids[3500:])\n",
    "    print np_ids[false_neg]\n",
    "    #print lm.coef_[0]  \n",
    "\n",
    "    \n",
    "trainKNN(49)\n",
    "#findK()\n",
    "trainBern()\n",
    "#bernParams()\n",
    "trainNumLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1631, 36)\n"
     ]
    }
   ],
   "source": [
    "# now import the test data\n",
    "\n",
    "# lists for storing labels, request text, metadata for each request, and request ids\n",
    "t_text = []\n",
    "t_data = []\n",
    "test_id = []\n",
    "count = 0\n",
    "\n",
    "# open this file for reading\n",
    "f = open(\"test.json\", \"r\")\n",
    "\n",
    "try:\n",
    "    data = json.load(f)\n",
    "    \n",
    "    for item in data:\n",
    "        \n",
    "        test_id.append(item['request_id'])\n",
    "        \n",
    "        # get the text of request\n",
    "        words = unicode(item['request_text_edit_aware'])\n",
    "        \n",
    "        # remove links and replace with string \"hyperlink.\" this prevents lots of spurious tokens\n",
    "        words = links.sub(u'hyperlink', words)\n",
    "        \n",
    "        # tokenize and lemmatize\n",
    "        tokens = [t for t in tokenizer.tokenize(words) if t.lower() not in sw]\n",
    "        tokens = [wl.lemmatize(t.lower()) for t in tokens]\n",
    "\n",
    "        # convert tokens back to string and store it\n",
    "        t_text.append(' '.join(tokens))\n",
    "        \n",
    "        # get the title of request, tokenize, stem/lemmatize\n",
    "        title_words = unicode(item['request_title'])\n",
    "        title_tokens = [t for t in tokenizer.tokenize(title_words) if t.lower() not in sw]\n",
    "        title_tokens = [wl.lemmatize(t.lower()) for t in title_tokens]\n",
    "        \n",
    "        # combine tokens from text and title\n",
    "        tokens += title_tokens\n",
    "        \n",
    "        # extract metadata. store data for each request in num\n",
    "        num = []\n",
    "        num.append(item['requester_account_age_in_days_at_request']/100.0)\n",
    "        num.append(item['requester_days_since_first_post_on_raop_at_request'])\n",
    "        num.append(item['requester_number_of_comments_at_request']/100.0)\n",
    "        num.append(int(item['requester_number_of_comments_in_raop_at_request'] > 0))\n",
    "        num.append(item['requester_number_of_posts_at_request'])\n",
    "        num.append(item['requester_number_of_subreddits_at_request'])\n",
    "        num.append(int(item['requester_number_of_posts_on_raop_at_request'] > 0))\n",
    "        num.append(item['requester_upvotes_minus_downvotes_at_request'])\n",
    "        num.append(item['requester_upvotes_plus_downvotes_at_request'])\n",
    "        \n",
    "        # store the number of tokens in the request and title as additional features\n",
    "        num.append(len(tokens))\n",
    "        num.append(len(title_tokens))\n",
    "        \n",
    "        # extract one feature for each word in the keyword array\n",
    "        for word in highprob:\n",
    "            num.append(int(word in tokens))\n",
    "        \n",
    "        # manually check for jpg in the request. ugly, but we already filtered urls\n",
    "        if 'jpg' in item['request_text_edit_aware']:\n",
    "            num.append(1)\n",
    "        else:\n",
    "            num.append(0)\n",
    "       \n",
    "        #num.append(int('hyperlink' in tokens))\n",
    "        \n",
    "        # get the day of the month request occurred. set feature to 1 if in first half of month\n",
    "        # additional experimentation shows that this is the best day to select\n",
    "        dt = int(datetime.datetime.fromtimestamp(item['unix_timestamp_of_request_utc']).strftime('%d'))\n",
    "        num.append(int(dt <= 15))\n",
    "            \n",
    "        # append the entire list to num_data, which stores data for all examples\n",
    "        t_data.append(num)\n",
    "        count += 1\n",
    "\n",
    "except Exception, e:\n",
    "    print \"error reading from file: %s\" %e\n",
    "    f.close()\n",
    "\n",
    "f.close()\n",
    "\n",
    "# convert python lists to numpy arrays\n",
    "# _data contains the text data, and _nums contains the metadata\n",
    "test_text = np.array(t_text)\n",
    "test_data = np.array(t_data)\n",
    "print test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4040, 36)\n",
      "Writing to file\n"
     ]
    }
   ],
   "source": [
    "# run models on the test data and export to csv file for submission\n",
    "\n",
    "# store the full training data in numpy array\n",
    "full_labels = np.array(labels)\n",
    "full_train = np.array(num_data)\n",
    "\n",
    "print full_train.shape\n",
    "\n",
    "'''# run the best model on the test data\n",
    "bern = BernoulliNB(alpha=20.0, binarize=0.9)\n",
    "bern.fit(full_train, full_labels)\n",
    "#pred_labels = bern.predict(test_data)\n",
    "pred_labels = bern.predict_proba(test_data)'''\n",
    "\n",
    "lm = LogisticRegression()\n",
    "lm.fit(full_train, full_labels)\n",
    "#pred_labels = lm.predict(test_data)\n",
    "pred_labels = lm.predict_proba(test_data)\n",
    "\n",
    "# eric's code to write to csv\n",
    "if len(test_id) == pred_labels.shape[0]:\n",
    "    print \"Writing to file\"\n",
    "    outfile = open(\"output.csv\",'w')\n",
    "    outfile.write(\"request_id,requester_received_pizza\\n\")\n",
    "    for i in range(pred_labels.shape[0]):\n",
    "        outfile.write(test_id[i]+','+str(pred_labels[i][1])+'\\n')\n",
    "    outfile.close()\n",
    "else:\n",
    "    print \"Prediction dimension mismatch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
