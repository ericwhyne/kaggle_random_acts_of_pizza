{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Acts of Pizza Kaggle Demo\n",
    "\n",
    "###James Route, Eric Whyne, Raymond Buhr, Filip Krunic\n",
    "This project is based on the [Random Acts of Pizza](https://www.kaggle.com/c/random-acts-of-pizza) Kaggle competition. The competition is derived from Reddit's Random Acts of Pizza board, where users post a request and short message in hopes that someone else will read it and order them a pizza. The competition's objective is to accurately predict whether a post will successfully receive a pizza using a limited amount of data: the text of the post and title, and several metadata fields such as the post's timestamp and the number of posts the user has made.\n",
    "\n",
    "We are given approximately 5500 sample posts in JSON format to work with. Of these, 4000 are reserved for training and the remaining 1500 are for testing. The test set has fewer data fields and does not include a field with the correct answers, so for much of this notebook we reserve about 500 samples in a dev set for checking accuracy.\n",
    "\n",
    "To start with, we import all of the libraries we'll need. This notebook assumes that the files containing training and test data (train.json and test.json) are located in the same directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# General libraries for reading and manipulating data\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "import datetime\n",
    "from collections import Counter\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.mixture import GMM\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "# import nltk functions for manipulating text\n",
    "import nltk\n",
    "from nltk.tokenize.regexp import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from file and function for Kaggle format\n",
    "\n",
    "In this block of code we set up data structures, divide the data into train and dev sets, and create a function to write files to the kaggle format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "orig_train = json.loads(open(\"train.json\").read())\n",
    "orig_test = json.loads(open(\"test.json\").read())\n",
    "\n",
    "all_labels = []\n",
    "for item in orig_train:\n",
    "    all_labels.append(int(item['requester_received_pizza']))\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "\n",
    "# Create a function for writing solutions to a file in kaggle format\n",
    "def write_solution_to_file(prediction, filename):\n",
    "    if len(prediction) == len(kaggle_test_id):\n",
    "        print \"Writing to file\", filename\n",
    "        outfile = open(filename,'w')\n",
    "        outfile.write(\"request_id,requester_received_pizza\\n\")\n",
    "        for i in range(0,len(prediction)):\n",
    "            outfile.write(kaggle_test_id[i]+','+str(prediction[i])+'\\n')\n",
    "        return 1\n",
    "    else:\n",
    "        print \"Prediction dimension mismatch\"\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.396039604 % of requestors received pizza in our training set.\n",
      "We could probably get an interesting score just by submitting zeros. Let's do that first.\n",
      "Writing to file submission_zeros.csv\n",
      "Submitting all zeros to Kaggle resulted in a 0.50000 score.\n"
     ]
    }
   ],
   "source": [
    "print 100 - sum(all_labels) / float(len(all_labels)) * 100, \"% of requestors received pizza in our training set.\"\n",
    "print \"We could probably get an interesting score just by submitting zeros. Let's do that first.\"\n",
    "write_solution_to_file(np.zeros(len(orig_test)),\"submission_zeros.csv\")\n",
    "print \"Submitting all zeros to Kaggle resulted in a 0.50000 score.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that submitting all zeros to Kaggle returned a score of .5 tells us that their test data set is comprised of examples evenly split between receiving and not receiving pizza. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Part 1: Initial model using text-based approach\n",
    "We make our first attempt at training a model for prediction. Since this is a text-heavy task, it seems reasonable to try a bag-of-words model using a countVectorizer for feature transformation and logistic regression for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# these lists store the text from each request and the outcome of the request\n",
    "text = []\n",
    "\n",
    "for item in orig_train:\n",
    "    # concatenate the text and title, then take all unique tokens\n",
    "    text.append(item['request_text_edit_aware'] + ' ' + item['request_title'])\n",
    "    \n",
    "f.close()\n",
    "\n",
    "# separate data into training and dev sets. this gives us ~500 samples in dev\n",
    "train_labels = np.array(all_labels[:3500])\n",
    "train_text = np.array(text[:3500])\n",
    "\n",
    "dev_labels = np.array(all_labels[3500:])\n",
    "dev_text = np.array(text[3500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3500, 12319) (540, 12319)\n",
      "base f1 = 0.25\n",
      "0.700\n"
     ]
    }
   ],
   "source": [
    "# set up a basic vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(train_text)\n",
    "Y = vectorizer.transform(dev_text)    \n",
    "print X.shape, Y.shape\n",
    "    \n",
    "# fit a logistic regression model and test accuracy on dev\n",
    "lm = LogisticRegression()\n",
    "lm.fit(X, train_labels)\n",
    "pred_labels = lm.predict(Y)\n",
    "    \n",
    "score = metrics.f1_score(dev_labels, pred_labels)\n",
    "print \"base f1 =\", score\n",
    "\n",
    "correct = (pred_labels == dev_labels)\n",
    "accuracy = 1.0 * np.sum(correct) / len(pred_labels)\n",
    "print \"%.03f\" %accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 2: Switching text method for numerical data\n",
    "We can try to reduce the vocabulary size to make this model more generalizable, using stemming/lemmatization, stopword elimination, filtering with regular expressions, etc. We can also try other models, such as Naive Bayes. However, none of these offer big improvements.\n",
    "\n",
    "Therefore, a purely text-based approach must not be the answer. This time we try using the numerical metadata in each of the samples. Note that we only use the fields that are available in both the training and test files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training and dev sets shape: (3500, 9) (540, 9)\n",
      "Kaggle test shape (1631, 9)\n"
     ]
    }
   ],
   "source": [
    "def meta_to_train(data):\n",
    "    # store the metadata fields in each sample\n",
    "    metadata = []\n",
    "    for item in data:   \n",
    "        # extract metadata. store data for each post in temporary list\n",
    "        sample_data = []\n",
    "        sample_data.append(item['requester_account_age_in_days_at_request'])\n",
    "        sample_data.append(item['requester_days_since_first_post_on_raop_at_request'])\n",
    "        sample_data.append(item['requester_number_of_comments_at_request'])\n",
    "        sample_data.append(item['requester_number_of_comments_in_raop_at_request'])\n",
    "        sample_data.append(item['requester_number_of_posts_at_request'])\n",
    "        sample_data.append(item['requester_number_of_subreddits_at_request'])\n",
    "        sample_data.append(item['requester_number_of_posts_on_raop_at_request'])\n",
    "        sample_data.append(item['requester_upvotes_minus_downvotes_at_request'])\n",
    "        sample_data.append(item['requester_upvotes_plus_downvotes_at_request'])   \n",
    "        metadata.append(sample_data)\n",
    "    return np.array(metadata)\n",
    "\n",
    "orig_train_metadata = meta_to_train(orig_train)\n",
    "kaggle_test_metadata = meta_to_train(orig_test)\n",
    "\n",
    "train_metadata = orig_train_metadata[:3500]\n",
    "dev_metadata = orig_train_metadata[3500:]\n",
    "print \"Our training and dev sets shape:\", train_data.shape, dev_data.shape\n",
    "print \"Kaggle test shape\", kaggle_test_metadata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try a few simple models and see how the performance looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-nearest neighbors:\n",
      "0.756\n",
      "base f1 = 0.0149253731343\n",
      "Logistic regression:\n",
      "0.757\n",
      "base f1 = 0.0839160839161\n",
      "Naive Bayes:\n",
      "0.717\n",
      "base f1 = 0.274881516588\n"
     ]
    }
   ],
   "source": [
    "# train k-nearest numbers model using the metadata\n",
    "def trainKNN(k=1):\n",
    "    model = KNeighborsClassifier(n_neighbors=k)\n",
    "    model.fit(train_metadata, train_labels)\n",
    "    pred_labels = model.predict(dev_metadata)\n",
    "    \n",
    "    correct = (pred_labels == dev_labels)\n",
    "    accuracy = 1.0 * np.sum(correct) / len(pred_labels)\n",
    "    print \"K-nearest neighbors:\"\n",
    "    print \"%.03f\" %accuracy\n",
    "    \n",
    "    score = metrics.f1_score(dev_labels, pred_labels)\n",
    "    print \"base f1 =\", score\n",
    "\n",
    "# train logistic regression using the metadata\n",
    "def trainLM():\n",
    "    lm = LogisticRegression()\n",
    "    lm.fit(train_metadata, train_labels)\n",
    "    pred_labels = lm.predict(dev_metadata)\n",
    "    \n",
    "    correct = (pred_labels == dev_labels)\n",
    "    accuracy = 1.0 * np.sum(correct) / len(pred_labels)\n",
    "    print \"Logistic regression:\"\n",
    "    print \"%.03f\" %accuracy\n",
    "    \n",
    "    score = metrics.f1_score(dev_labels, pred_labels)\n",
    "    print \"base f1 =\", score\n",
    "\n",
    "# train a simple BernoulliNB model\n",
    "def trainNB(a=1.0):\n",
    "    bern = BernoulliNB(alpha=a)\n",
    "    bern.fit(train_metadata, train_labels)\n",
    "    pred_labels = bern.predict(dev_metadata)\n",
    "    \n",
    "    correct = (pred_labels == dev_labels)\n",
    "    accuracy = 1.0 * np.sum(correct) / len(pred_labels)\n",
    "    print \"Naive Bayes:\"\n",
    "    print \"%.03f\" %accuracy\n",
    "    \n",
    "    score = metrics.f1_score(dev_labels, pred_labels)\n",
    "    print \"base f1 =\", score\n",
    "\n",
    "trainKNN(k=20)\n",
    "trainLM()\n",
    "trainNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to file metadata_logistic_regression_submission.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic regression looked ok, so let's make a Kaggle submission based on that\n",
    "lm = LogisticRegression()\n",
    "lm.fit(orig_train_metadata, all_labels) # we want to train with all the data we have when preparing a submission\n",
    "kaggle_metadata_pred_labels = lm.predict(kaggle_test_metadata)\n",
    "write_solution_to_file(kaggle_metadata_pred_labels, \"submission_metadata_logistic_regression.csv\")\n",
    "\n",
    "# This scored 0.51161 according to kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 3: Engineering new features for a combined approach\n",
    "The accuracy of these models is higher, but only NB offers an improvement in f-score. Let's try to get more creative with our feature set and combine some of the text-based features with the metadata. See below for an explanation of the new fields:\n",
    "\n",
    "* Extraction of keywords as features. A paper accompanying the Kaggle competition from Althoff, et al performed some topic modeling and found sets of keywords that improved prediction accuracy. We implemented these and found slight improvements. What worked better was using Bayes' theorem to find general keywords predictive of pizza. See the appendix for details on what we did.\n",
    "* Also from Althoff et al, a request in the first half of a month tends to do better. We code this as a binary feature.\n",
    "* We code the length of the request and the title (in tokens) as features.\n",
    "* We check the original text file to see if 'jpg' is present, an indicator that the user linked to an image. Note that other image types didn't yield predictive power.\n",
    "* We code the presence of a hyperlink as another feature\n",
    "* We convert some of the numerical features to binary or divide by 100 to reduce their size, which improves accuracy. We originally tried normalizing all data so it would fall between 0 and 1, which reduced accuracy. We also tried a log transform to correct high skew in some of the fields, which also didn't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info http://www.nltk.org/nltk_data/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the nltk downloader and select stopwords wordnet from the corpora section.  \n",
    "nltk.download()\n",
    "\n",
    "# Be sure to close the downloader window before executing the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3500, 86) (540, 86)\n"
     ]
    }
   ],
   "source": [
    "def train_to_combined(data):\n",
    "    # keywords to extract as features\n",
    "    highprob = [u'surviving', u'stretch', u'married', u'total', u'incredibly', u'landlord', \n",
    "            u'cover', u'receiving', u'stopped', u'nj', u'bare', u'current', u'jpg', \n",
    "            u'exchange', u'normally', u'applied', u'checks', u'truly', u'heat', u'aren', \n",
    "            u'plain', u'kentucky', u'reasons', u'father', u'rice', u'including', u'talking', \n",
    "            u'especially', u'second', u'costs', u'july', u'lift', u'posted', u'considering', \n",
    "            u'eggs', u'generosity', u'meals', u'certainly', u'bucks', u'mac', u'fall', \n",
    "            u'rather', u'receive', u'deal', u'aid', u'tight', u'imgur', u'lots', u'tough', \n",
    "            u'eternally', u'f', u'expenses', u'beans', u'savings', u'visit', u'daughter', \n",
    "            u'accident', u'call', u'form', u'boys', u'available', u'seem', u'checking', \n",
    "            u'details', u'cheap', u'financially', u'assistance', u'non', u'unexpected', \n",
    "            u'sunday', u'oatmeal', u'pic']\n",
    "\n",
    "    # list to store extracted features\n",
    "    metadata = []\n",
    "\n",
    "    # tokenize using this regex\n",
    "    tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "\n",
    "    # lemmatize the tokens\n",
    "    wl = WordNetLemmatizer()\n",
    "\n",
    "    # filter hyperlinks with this regex\n",
    "    links = re.compile(r'\\bhttp\\S+\\b')\n",
    "\n",
    "    # stopwords to filter with nltk\n",
    "    sw = stopwords.words('english')\n",
    "\n",
    "    for item in data:\n",
    "        \n",
    "        # store data for each sample in temporary list\n",
    "        num = []\n",
    "        \n",
    "        # get the text of request\n",
    "        words = unicode(item['request_text_edit_aware'])\n",
    "        \n",
    "        # remove links and replace with string \"hyperlink.\" this prevents lots of spurious tokens\n",
    "        words = links.sub(u'hyperlink', words)\n",
    "        \n",
    "        # remove stopwords, tokenize, lemmatize\n",
    "        tokens = [t for t in tokenizer.tokenize(words) if t.lower() not in sw]\n",
    "        tokens = [wl.lemmatize(t.lower()) for t in tokens]\n",
    "\n",
    "        # convert tokens back to string and store it\n",
    "        #text.append(' '.join(tokens))\n",
    "        \n",
    "        # get the title of request, tokenize, stem/lemmatize\n",
    "        title_words = unicode(item['request_title'])\n",
    "        title_tokens = [t for t in tokenizer.tokenize(title_words) if t.lower() not in sw]\n",
    "        title_tokens = [wl.lemmatize(t.lower()) for t in title_tokens]\n",
    "        \n",
    "        # store the number of tokens in the request and title as additional features\n",
    "        num.append(len(tokens))\n",
    "        num.append(len(title_tokens))\n",
    "    \n",
    "        # combine tokens from text and title\n",
    "        tokens += title_tokens\n",
    "        \n",
    "        # extract metadata. store data for each request in list\n",
    "        num.append(item['requester_account_age_in_days_at_request']/100.0)\n",
    "        num.append(item['requester_days_since_first_post_on_raop_at_request'])\n",
    "        num.append(item['requester_number_of_comments_at_request']/100.0)\n",
    "        num.append(int(item['requester_number_of_comments_in_raop_at_request'] > 0))\n",
    "        num.append(item['requester_number_of_posts_at_request'])\n",
    "        num.append(item['requester_number_of_subreddits_at_request'])\n",
    "        num.append(int(item['requester_number_of_posts_on_raop_at_request'] > 0))\n",
    "        num.append(item['requester_upvotes_minus_downvotes_at_request'])\n",
    "        num.append(item['requester_upvotes_plus_downvotes_at_request'])\n",
    "        \n",
    "        # extract one feature for each word in the keyword array\n",
    "        for word in highprob:\n",
    "            num.append(int(word in tokens))\n",
    "        \n",
    "        # manually check for jpg in the request, since we already filtered urls\n",
    "        if 'jpg' in item['request_text_edit_aware']:\n",
    "            num.append(1)\n",
    "        else:\n",
    "            num.append(0)\n",
    "        \n",
    "        # if the text contains a hyperlink, set feature to 1\n",
    "        num.append(int('hyperlink' in tokens))\n",
    "       \n",
    "        # get the day of the month request occurred. set feature to 1 if in first half of month\n",
    "        dt = int(datetime.datetime.fromtimestamp(item['unix_timestamp_of_request_utc']).strftime('%d'))\n",
    "        num.append(int(dt <= 15))\n",
    "                \n",
    "        # append the entire list to num_data, which stores data for all examples\n",
    "        metadata.append(num)\n",
    "    return np.array(metadata)\n",
    "\n",
    "combined_features = train_to_combined(orig_train)\n",
    "kaggle_test_combined_features = train_to_combined(orig_test)\n",
    "\n",
    "train_data = combined_features[:3500]\n",
    "dev_data = combined_features[3500:]\n",
    "print train_data.shape, dev_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###3.1: Optimizing models for better performance\n",
    "Next, we try running a set of algorithms again and compare the scores. Note that the acccuracy and f1 for the regression continues to rise, as does the f1 for Naive Bayes. We further increase this by using gridsearch to optimize parameter values for some of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN\n",
      "the best parameter is k=49.000000\n",
      "\n",
      "K-nearest neighbors:\n",
      "0.757\n",
      "base f1 = 0.0\n",
      "\n",
      "NB\n",
      "The best parameter is alpha=20.000000\n",
      "\n",
      "Naive Bayes:\n",
      "0.717\n",
      "base f1 = 0.274881516588\n"
     ]
    }
   ],
   "source": [
    "# run gridsearch to find best k value\n",
    "def findK():\n",
    "    # range is from an iterative process to narrow down\n",
    "    params = {'n_neighbors': range(38,56,1)}\n",
    "    search = GridSearchCV(KNeighborsClassifier(), params)\n",
    "    search.fit(train_data, train_labels)\n",
    "    \n",
    "    print \"the best parameter is k=%f\\n\" %search.best_params_['n_neighbors']\n",
    "    #print \"summary of all params:\\n\", search.grid_scores_\n",
    "\n",
    "def bernParams():\n",
    "    #params = {'binarize': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}\n",
    "    params = {'alpha': [0.0, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0, 20.0, 50.0]}\n",
    "    search = GridSearchCV(BernoulliNB(), params)\n",
    "    search.fit(train_data, train_labels)\n",
    "    \n",
    "    print \"The best parameter is alpha=%f\\n\" %search.best_params_['alpha']\n",
    "    #print \"summary of all params:\\n\", search.grid_scores_\n",
    "\n",
    "print \"KNN\"\n",
    "findK()\n",
    "trainKNN(k=49)\n",
    "\n",
    "print \"\\nNB\"\n",
    "bernParams()\n",
    "trainNB(a=20.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to file submission_knn49.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a sumbission using KNN where K=49\n",
    "model = KNeighborsClassifier(n_neighbors=49)\n",
    "model.fit(orig_train_metadata, all_labels)\n",
    "kaggle_metadata_pred_labels = model.predict(kaggle_test_metadata)\n",
    "\n",
    "write_solution_to_file(kaggle_metadata_pred_labels, \"submission_knn49.csv\")\n",
    "\n",
    "# This solution produced a result of 0.50124 on kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for RandomForest is 0.750\n",
      "accuracy for RBM + SVM is 0.757\n",
      "accuracy for AdaBoostClassifier is 0.757\n",
      "accuracy for PCA + RandomForest is 0.700\n"
     ]
    }
   ],
   "source": [
    "def trainAB(): \n",
    "    model = AdaBoostClassifier()\n",
    "    model.fit(train_data, train_labels)\n",
    "    pred_labels = model.predict(dev_data)\n",
    "    \n",
    "    # display the accuracy\n",
    "    correct = (pred_labels == dev_labels)\n",
    "    accuracy = 1.0 * np.sum(correct) / len(dev_labels)\n",
    "    print \"accuracy for AdaBoostClassifier is %.03f\" %accuracy\n",
    "    \n",
    "# PCA randomForest pipeline\n",
    "def pcaTrainRF():\n",
    "    model = RandomForestClassifier()\n",
    "    pca = PCA(n_components = 5, whiten=True)\n",
    "    pipe = Pipeline(steps=[('pca', pca), ('RF', model)])\n",
    "    pipe.fit(train_data, train_labels)\n",
    "    pred_labels = pipe.predict(dev_data)\n",
    "    \n",
    "    # display the accuracy\n",
    "    correct = (pred_labels == dev_labels)\n",
    "    accuracy = 1.0 * np.sum(correct) / len(dev_labels)\n",
    "    print \"accuracy for PCA + RandomForest is %.03f\" %accuracy\n",
    "    \n",
    "# BernoulliRBM + logistic regression\n",
    "def rbmLogistic():\n",
    "    model = SVC()\n",
    "    rbm = BernoulliRBM(random_state=0, verbose=False)\n",
    "    rbm.learning_rate=0.86\n",
    "    rbm.n_iter = 20\n",
    "    rbm.n_components = 100\n",
    "    pipe = Pipeline(steps=[('rbm', rbm), ('model', model)])\n",
    "    pipe.fit(train_data, train_labels)\n",
    "    pred_labels = pipe.predict(dev_data)\n",
    "    # display the accuracy\n",
    "    correct = (pred_labels == dev_labels)\n",
    "    accuracy = 1.0 * np.sum(correct) / len(dev_labels)\n",
    "    print \"accuracy for RBM + SVM is %.03f\" %accuracy    \n",
    "\n",
    "# random forest classifier \n",
    "def trainRF():\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(train_data, train_labels)\n",
    "    pred_labels = model.predict(dev_data)\n",
    "    \n",
    "    # display the accuracy\n",
    "    correct = (pred_labels == dev_labels)\n",
    "    accuracy = 1.0 * np.sum(correct) / len(dev_labels)\n",
    "    print \"accuracy for RandomForest is %.03f\" %accuracy\n",
    "    \n",
    "trainRF()\n",
    "rbmLogistic()\n",
    "trainAB()\n",
    "pcaTrainRF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Appendix A: Generating keywords from text data\n",
    "We initially worked with the idea from Althouse, et al and checked for the presence of words from among several lists. This only succeeded increasing accuracy a marginal amount, so we decided to derive an approach using Bayes' Theorem, determining which words in the entire corpus are more strongly associated with receiving pizza. Below is the code use to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_count = Counter() # holds number of successful requests that contain a given word\n",
    "w_count = Counter() # holds number of total requests that contain a given word\n",
    "probs = [] # holds the probabilities associated with each word\n",
    "total = 0\n",
    "\n",
    "f = open(\"train.json\", \"r\")\n",
    "#tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "#sw = stopwords.words('english')\n",
    "for item in data:\n",
    "        \n",
    "    # extract the outcome labels and text for request and title\n",
    "    receive = int(item['requester_received_pizza'])\n",
    "    words = item['request_text_edit_aware'] + ' ' + item['request_title']\n",
    "    tokens = set([t.lower() for t in tokenizer.tokenize(words) if t.lower() not in sw])\n",
    "        \n",
    "    if receive == 1:\n",
    "        p_count.update(t for t in tokens)\n",
    "        \n",
    "    w_count.update(t for t in tokens)\n",
    "    total += 1\n",
    "\n",
    "f.close()\n",
    "\n",
    "# store results in tuple. contents are the word, occurrences in successful requests, ratio of\n",
    "# occurrences in successful request to total occurrences\n",
    "for key, value in p_count.iteritems():\n",
    "    prob = 1.0 * value / w_count.get(key)\n",
    "    probs.append((key, value, prob))\n",
    "\n",
    "probs.sort(key=lambda tup: tup[2], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at the sorted list and pick out the terms that are most strongly associated with receiving pizza, based on number of occurrances and probability. Setting lower thresholds tends to yield the best results on the dev data, but when submitting to Kaggle, more restrictive lists work better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'cover', u'current', u'jpg', u'exchange', u'normally', u'aren', u'father', u'rice', u'especially', u'second', u'posted', u'considering', u'meals', u'bucks', u'rather', u'receive', u'deal', u'aid', u'tight', u'imgur', u'tough', u'expenses', u'beans', u'visit', u'daughter', u'call', u'checking', u'details', u'cheap', u'financially', u'assistance', u'sunday']\n"
     ]
    }
   ],
   "source": [
    "# print contents of the tuple list based on conditions\n",
    "wordlist = []\n",
    "\n",
    "for item in probs:\n",
    "    # print tuple if word occurs in at least 10 successful requests and 45% of occurrences are in successful requests\n",
    "    if item[1] >= 15 and item[2] >= .4:\n",
    "        #print item\n",
    "        wordlist.append(item[0])\n",
    "\n",
    "# this just prints the words. handy if you want to paste list directly into code\n",
    "print wordlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Appendix B: The cutting room floor\n",
    "There were a number of features and other experiments we tried that didn't work out. These are listed here to give a sense of the other things we tried that didn't positively impact prediction accuracy:\n",
    "\n",
    "* Normalizing numeric data so it is contained on the interval [0,1]\n",
    "* Using log and inverse transforms to correct skew in numeric features\n",
    "* Extracting other features from the timestamp, such as time, day of the week, month, etc.\n",
    "* Using textBlob to calculate sentiment analysis and code it as a feature\n",
    "* Using NLTK to perform part-of-speech tagging and coding occurrences of certain words as features (proper nouns, adjectives, adverbs, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
