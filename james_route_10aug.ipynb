{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "import json\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.mixture import GMM\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "# import nltk functions for manipulating text. textblob for sentiment analysis\n",
    "import nltk\n",
    "from nltk.tokenize.regexp import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "#from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keywords identified in research paper linked from kaggle page\n",
    "\n",
    "money1 = ['week', 'ramen', 'paycheck', 'work', 'couple', 'rice', 'check', 'pizza', 'grocery', 'rent', 'anyone', 'favor', 'someone', 'bill', 'money']\n",
    "money2 = ['food', 'house', 'rent', 'stamp', 'month', 'today', 'parent', 'help', 'anything', 'mom', 'anyone']\n",
    "job = ['job', 'year', 'interview', 'luck', 'school', 'paycheck', 'unemployment', 'end']\n",
    "student = ['student', 'college', 'final', 'loan', 'summer', 'university', 'class', 'meal', 'semester', 'story', 'kid']\n",
    "time_family = ['tonight', 'night', 'tomorrow', 'friday', 'dinner', 'something', 'account', 'family', 'bank', 'home']\n",
    "time = ['day', 'pay']\n",
    "gratitude = ['thanks', 'advance', 'guy', 'reading', 'place', 'everyone', 'craving', 'kind']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4040\n",
      "(3500, 56)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'for i in range(11):\\n    max1 = np.mean(train_nums[:,i])\\n    max2 = np.mean(dev_nums[:,i])\\n    \\n    train_nums[:,i] = train_nums[:,i]/max1\\n    dev_nums[:,i] = dev_nums[:,i]/max2'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#os.environ[\"NLTK_DATA\"] = \"~/nltk_data\"\n",
    "# stopwords to filter with nltk\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "# tokenize using this regex\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "\n",
    "# filter hyperlinks with this regex\n",
    "links = re.compile(r'\\bhttp\\S+\\b')\n",
    "\n",
    "# lemmatize the tokens\n",
    "wl = WordNetLemmatizer()\n",
    "#sn = SnowballStemmer(\"english\")\n",
    "\n",
    "# lists for storing labels, request text, metadata for each request, and request ids\n",
    "labels = []\n",
    "text = []\n",
    "num_data = []\n",
    "ids = []\n",
    "count = 0\n",
    "\n",
    "# open this file for reading\n",
    "f = open(\"train.json\", \"r\")\n",
    "\n",
    "try:\n",
    "    data = json.load(f)\n",
    "    \n",
    "    for item in data:\n",
    "        \n",
    "        prop = 0\n",
    "        adj = 0\n",
    "        adv = 0\n",
    "        # extract the outcome labels\n",
    "        labels.append(int(item['requester_received_pizza']))\n",
    "        \n",
    "        # keep track of the request ids. useful for debugging\n",
    "        ids.append(item['request_id'])\n",
    "        \n",
    "        # get the text of request\n",
    "        words = unicode(item['request_text_edit_aware']) #item['request_title']#\n",
    "        \n",
    "        # remove links and replace with string \"hyperlink.\" this prevents lots of spurious tokens\n",
    "        words = links.sub(u'hyperlink', words)\n",
    "        \n",
    "        # tokenize and stem/lemmatize\n",
    "        tokens = [t for t in tokenizer.tokenize(words) if t.lower() not in sw]\n",
    "        tokens = [wl.lemmatize(t.lower()) for t in tokens]\n",
    "\n",
    "        \n",
    "        #tokens = [sn.stem(t) for t in tokens]\n",
    "        # convert tokens back to string and store it\n",
    "        text.append(' '.join(tokens))\n",
    "        \n",
    "        # get the title of request, tokenize, stem/lemmatize\n",
    "        title_words = item['request_title']\n",
    "        title_tokens = [t for t in tokenizer.tokenize(title_words) if t.lower() not in sw]\n",
    "        title_tokens = [wl.lemmatize(t) for t in title_tokens]\n",
    "        \n",
    "        # extract metadata. store data for each request in num\n",
    "        num = []\n",
    "        num.append(item['requester_account_age_in_days_at_request']/100.0)\n",
    "        num.append(item['requester_days_since_first_post_on_raop_at_request'])\n",
    "        num.append(item['requester_number_of_comments_at_request']/100.0)\n",
    "        num.append(item['requester_number_of_comments_in_raop_at_request'])\n",
    "        num.append(item['requester_number_of_posts_at_request'])\n",
    "        num.append(item['requester_number_of_subreddits_at_request'])\n",
    "        num.append(item['requester_number_of_posts_on_raop_at_request'])\n",
    "        num.append(item['requester_upvotes_minus_downvotes_at_request'])\n",
    "        num.append(item['requester_upvotes_plus_downvotes_at_request'])\n",
    "        \n",
    "        # store the number of tokens in the request and title as additional features\n",
    "        num.append(len(tokens))\n",
    "        num.append(len(title_tokens))\n",
    "        \n",
    "        # extract one feature for each word in the keyword arrays\n",
    "        # some arrays aren't helpful so we skip them\n",
    "        for word in money1:\n",
    "            num.append(int(word in tokens))\n",
    "            \n",
    "        for word in money2:\n",
    "            num.append(int(word in tokens))\n",
    "            \n",
    "        for word in gratitude:\n",
    "            num.append(int(word in tokens))\n",
    "            \n",
    "        for word in time_family:\n",
    "            num.append(int(word in tokens))\n",
    "        \n",
    "        num.append(int('hyperlink' in tokens))\n",
    "        \n",
    "        # we can't use these features directly. they're not in the test data\n",
    "        '''flair = item['requester_user_flair']\n",
    "        num.append(item['number_of_downvotes_of_request_at_retrieval'])\n",
    "        num.append(item['number_of_upvotes_of_request_at_retrieval'])\n",
    "        num.append(item['request_number_of_comments_at_retrieval'])'''  \n",
    "        \n",
    "        # perform sentiment analysis (returns signed float). doesn't seem to affect accuracy\n",
    "        '''blob = TextBlob(unicode(words))\n",
    "        num.append(blob.sentiment.polarity)'''\n",
    "        \n",
    "        # calculate lexical diversity for text and title. doesn't seem to raise accuracy\n",
    "        '''# store the lexical diversity of the request as a feature\n",
    "        if len(tokens) > 0:\n",
    "            num.append(1.0*len(set(tokens))/len(tokens))\n",
    "        else:\n",
    "            num.append(0)\n",
    "        \n",
    "        # store the lexical diversity of the title as a feature\n",
    "        if len(title_tokens) > 0:\n",
    "            num.append(1.0*len(set(title_tokens))/len(title_tokens))\n",
    "        else:\n",
    "            num.append(0)'''\n",
    "        \n",
    "        # extract features from timestamp. appears to decrease accuracy \n",
    "        # get the timestamp of request, convert to datetime, and save off the hour\n",
    "        '''dt = -4 + int(datetime.datetime.fromtimestamp(item['unix_timestamp_of_request_utc']).strftime('%H'))\n",
    "        # quick and dirty conversion to eastern daylight time\n",
    "        if dt <= 0:\n",
    "            dt += 24\n",
    "        \n",
    "        # if hour is in the morning\n",
    "        if dt >= 3 and dt < 11:\n",
    "            num.append(1)\n",
    "            num.append(0)\n",
    "            num.append(0)\n",
    "        \n",
    "        # if hour is in midday\n",
    "        elif dt >= 11 and dt < 18:\n",
    "            num.append(0)\n",
    "            num.append(1)\n",
    "            num.append(0)\n",
    "        \n",
    "        # if hour is at night\n",
    "        else:\n",
    "            num.append(0)\n",
    "            num.append(0)\n",
    "            num.append(1)'''\n",
    "        #num.append(int(dt))\n",
    "        '''for i in range(dt):\n",
    "            num.append(0)\n",
    "        num.append(1)\n",
    "        for i in range(6-dt):\n",
    "            num.append(0)'''\n",
    "        '''if flair == None:\n",
    "            num.append(0)\n",
    "            num.append(0)\n",
    "        elif flair == 'shroom':\n",
    "            num.append(1)\n",
    "            num.append(0)\n",
    "        elif flair == 'PIF':\n",
    "            num.append(0)\n",
    "            num.append(1)'''\n",
    "            \n",
    "        # append the entire list to num_data, which stores data for all examples\n",
    "        num_data.append(num)\n",
    "        count += 1\n",
    "\n",
    "except Exception, e:\n",
    "    print \"error reading from file: %s\" %e\n",
    "    f.close()\n",
    "\n",
    "f.close()\n",
    "print len(labels)\n",
    "\n",
    "# convert python lists to numpy arrays\n",
    "# _data contains the text data, and _nums contains the metadata\n",
    "train_labels = np.array(labels[:3500])\n",
    "train_data = np.array(text[:3500])\n",
    "train_nums = np.array(num_data[:3500])\n",
    "print train_nums.shape\n",
    "\n",
    "dev_labels = np.array(labels[3500:])\n",
    "dev_data = np.array(text[3500:])\n",
    "dev_nums = np.array(num_data[3500:])\n",
    "\n",
    "# normalize the numerical data\n",
    "'''for i in range(11):\n",
    "    max1 = np.mean(train_nums[:,i])\n",
    "    max2 = np.mean(dev_nums[:,i])\n",
    "    \n",
    "    train_nums[:,i] = train_nums[:,i]/max1\n",
    "    dev_nums[:,i] = dev_nums[:,i]/max2'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1196.00028571\n",
      "*\n",
      "hyperlink 0\n",
      "hyperlink\n",
      "(540, 59) (3500, 59)\n",
      "739.771006944\n",
      "(3500, 59)\n"
     ]
    }
   ],
   "source": [
    "# scratchpad\n",
    "#for i in range(58):\n",
    "    #print np.mean(train_nums[:,i], axis=0)\n",
    "\n",
    "print train_nums[:,7].mean()\n",
    "\n",
    "print '*'\n",
    "print dev_data[150], dev_labels[150]\n",
    "print text[3650]\n",
    "\n",
    "print dev_nums.shape, train_nums.shape\n",
    "print np.max(dev_nums[:,1])\n",
    "print train_nums.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3500, 92200) (540, 92200)\n",
      "base f1 = 0.214689265537\n",
      "money food -0.0326457190941\n",
      "pizza love -0.0326457190941\n",
      "forward money -0.0326457190941\n",
      "could make -0.0326457190941\n",
      "surprise -0.0326457190941\n",
      "Improved f1 = 0.226600985222\n",
      "ranch -0.0326457190941\n",
      "expected -0.0326457190941\n",
      "surprise -0.0326457190941\n",
      "relative -0.0326457190941\n",
      "mentioned -0.0326457190941\n"
     ]
    }
   ],
   "source": [
    "# skip this cell. its accuracy is not good.\n",
    "def better_preprocessor(s):\n",
    "\n",
    "    '''# remove links and replace with string \"hyperlink\"\n",
    "    links = re.compile(u'\\\\bhttp\\w+\\\\b')\n",
    "    # remove non-alphanumerics\n",
    "    non_alpha = re.compile(u'[\\W_]+', re.UNICODE)\n",
    "    # change all number sequences to single '0'\n",
    "    num = re.compile(u'[0-9]+', re.UNICODE)\n",
    "    # list of common English suffixes to remove\n",
    "    suff = re.compile(u'(al|ance|ence|dom|ed|er|or|ism|ist|ity|ty|ment|ship|sion|ing|s|ly|ation|tion|able|ible|ate|en|ify|fy|ize|ise|ful|ic|ical|ious|ous|ish|ive)\\\\b', re.UNICODE)\n",
    "    # list of common English prefixes to remove\n",
    "    pre = re.compile(u'\\\\b(an|ante|anti|auto|circum|co|com|con|contra|de|dis|en|ex|extra|hyper|in|im|il|ir|inter|intra|macro|micro|mid|mis|mono|non|over|post|pre|pro|re|semi|sub|super|trans|tri|un)', re.UNICODE)\n",
    "    # remove words with one or two characters only (treat as stopwords)\n",
    "    stop = re.compile(u'\\\\b([\\w]{1,2})\\\\b', re.UNICODE)\n",
    "\n",
    "    # apply filters\n",
    "    t = links.sub(u'hyperlink', s)\n",
    "    t = non_alpha.sub(u' ', t)\n",
    "    t = num.sub(u\" 0 \", t)\n",
    "    t = suff.sub(u' ', t)\n",
    "    t = pre.sub(u' ', t)\n",
    "    t = stop.sub(u' ', t)'''\n",
    "    \n",
    "    '''tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "    words = np.array_str(s)\n",
    "    print type(words), words\n",
    "    tokens = tokenizer.tokenize(words)\n",
    "    #tokens = [t for t in tokenizer.tokenize(words) if t.lower() not in sw]\n",
    "    # Stem\n",
    "    #tokens = [wl.lemmatize(t) for t in tokens]\n",
    "    tokens = [sn.stem(t) for t in tokens]\n",
    "    return tokens'''\n",
    "\n",
    "# vectorize text data and then train logistic regression\n",
    "# this isn't very accurate (~30%)\n",
    "def trainLM():\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", ngram_range=[1,2])\n",
    "    X = vectorizer.fit_transform(train_data)\n",
    "    Y = vectorizer.transform(dev_data)\n",
    "    vocab = vectorizer.get_feature_names()\n",
    "    \n",
    "    print X.shape, Y.shape\n",
    "    \n",
    "    lm = LogisticRegression()\n",
    "    lm.fit(X, train_labels)\n",
    "    pred_labels = lm.predict(Y)\n",
    "    \n",
    "    score = metrics.f1_score(dev_labels, pred_labels)\n",
    "    print \"base f1 =\", score\n",
    "    \n",
    "    maxind = np.fabs(lm.coef_[0]).argsort()[-5:]\n",
    "    for item in maxind:\n",
    "        print vocab[item], lm.coef_[0][i]\n",
    "    \n",
    "    # vectorize data using the better preprocessor\n",
    "    better_vect = CountVectorizer(preprocessor=None)\n",
    "    better_X = better_vect.fit_transform(train_data)\n",
    "    better_Y = better_vect.transform(dev_data)\n",
    "    vocab2 = better_vect.get_feature_names()\n",
    "    \n",
    "    # run logistic regression on this data and report f1 score\n",
    "    better_lm = LogisticRegression()\n",
    "    better_lm.fit(better_X, train_labels)\n",
    "    better_labels = better_lm.predict(better_Y)\n",
    "    print \"Improved f1 =\", metrics.f1_score(dev_labels, better_labels)\n",
    "    \n",
    "    maxind2 = better_lm.coef_[0].argsort()[-5:]\n",
    "    for item in maxind2:\n",
    "        print vocab2[item], lm.coef_[0][i]\n",
    "    \n",
    "trainLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.757\n",
      "false +, false - : 0, 131\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "accuracy for LM is 0.765\n",
      "false +, false - : 9, 118\n",
      "[u't3_uvoyu' u't3_1exnem' u't3_lcs2n' u't3_q21uq' u't3_132u9h' u't3_w4fdd'\n",
      " u't3_i3ms5' u't3_12p0tt' u't3_zpn24' u't3_1mjbbd' u't3_lcqaj' u't3_ibua5'\n",
      " u't3_qnocf' u't3_w5491' u't3_1ik128' u't3_w8mfc' u't3_ijcon' u't3_u69ow'\n",
      " u't3_hutzt' u't3_1chlic' u't3_ur5yf' u't3_z7mrn' u't3_1ihhx4' u't3_j8fnb'\n",
      " u't3_hucws' u't3_ojq68' u't3_12zfh3' u't3_sp8xg' u't3_pk4g1' u't3_ktlwy'\n",
      " u't3_1lojow' u't3_1dr9hu' u't3_j94no' u't3_ihorg' u't3_y9vxk' u't3_11lvl7'\n",
      " u't3_1l5cvv' u't3_qqotp' u't3_j6433' u't3_icpji' u't3_l9g0n' u't3_ikszl'\n",
      " u't3_1enrol' u't3_idifo' u't3_xtx62' u't3_1ifnr4' u't3_ilau5' u't3_uotiq'\n",
      " u't3_1jlnc4' u't3_iekd3' u't3_19gv1n' u't3_o8aso' u't3_o4fyx' u't3_m7to4'\n",
      " u't3_wgk03' u't3_l0vt2' u't3_18zn0d' u't3_17vccr' u't3_kfxsm' u't3_1j485g'\n",
      " u't3_kwqew' u't3_ibvsg' u't3_1jgj2w' u't3_js9be' u't3_ll3uk' u't3_182nt8'\n",
      " u't3_iqm6n' u't3_l7opc' u't3_tpezd' u't3_xd34k' u't3_1ich4u' u't3_152l21'\n",
      " u't3_rowqk' u't3_1bkedk' u't3_17z4w1' u't3_15uyq3' u't3_huzao' u't3_jp7f2'\n",
      " u't3_14sy0u' u't3_n955o' u't3_w7yx5' u't3_jh8y1' u't3_ig6ya' u't3_ide15'\n",
      " u't3_v8h8a' u't3_183m94' u't3_1nphr4' u't3_u027s' u't3_kj1lv' u't3_198a81'\n",
      " u't3_nztz2' u't3_oqt9r' u't3_jxdjg' u't3_1czlly' u't3_nucpj' u't3_1d4ncf'\n",
      " u't3_jd4dp' u't3_m6ue5' u't3_19t34q' u't3_105fpq' u't3_16y5eo' u't3_ihosh'\n",
      " u't3_ionv5' u't3_1mwwu5' u't3_rn074' u't3_nef5s' u't3_11zb19' u't3_hj1hr'\n",
      " u't3_15ybss' u't3_1dnhcy' u't3_1eilsp' u't3_wyilc' u't3_uiv13' u't3_i5c8j'\n",
      " u't3_ibfwq' u't3_j06yw' u't3_penyh' u't3_oo525']\n",
      "[  2.78126039e-02   1.23885089e-03   3.74569593e-04   6.20356897e-02\n",
      "   1.33088841e-04  -1.04723942e-03   6.91644984e-01   9.08654810e-06\n",
      "  -7.18546402e-07   2.83364273e-03   1.06669836e-02   2.56357616e-01\n",
      "   2.18150197e-01   3.60584924e-01   7.79485511e-02  -7.88206443e-02\n",
      "   5.58622794e-01   4.68077928e-01   7.53819599e-02   2.25040380e-02\n",
      "  -6.80228776e-02   1.62186992e-01   3.07535904e-02   2.04416954e-02\n",
      "   1.38573285e-01   7.47570773e-02   1.94619328e-01  -2.78648671e-01\n",
      "  -6.80228776e-02   1.66681604e-02   1.44771674e-01   6.59925211e-02\n",
      "  -4.78151573e-02   9.38529591e-02   2.73159023e-02  -1.22968137e-01\n",
      "   1.62186992e-01   2.16907030e-01   1.41764933e-02  -1.23637067e-02\n",
      "  -1.87345539e-01  -3.05711244e-01   2.14749514e-01  -3.54513235e-02\n",
      "  -1.52950935e-02   2.15632355e-01  -1.84678794e-02  -3.11896702e-01\n",
      "   2.39786219e-01  -4.84463378e-02   2.49398736e-02   1.58242848e-01\n",
      "  -8.44550156e-02  -4.88370404e-02   6.21897671e-02   5.33987518e-01]\n"
     ]
    }
   ],
   "source": [
    "# train a simple KNN model and check accuracy\n",
    "# this uses the metadata\n",
    "def trainKNN(k=1):\n",
    "    # value for k from gridsearch output\n",
    "    model = KNeighborsClassifier(n_neighbors=38)\n",
    "    model.fit(train_nums, train_labels)\n",
    "    pred_labels = model.predict(dev_nums)\n",
    "    \n",
    "    # calculate accuracy and identify the false positive and negative cases\n",
    "    correct = (pred_labels == dev_labels)\n",
    "    false_pos = (pred_labels > dev_labels)\n",
    "    false_neg = (pred_labels < dev_labels)\n",
    "    accuracy = 1.0 * np.sum(correct) / len(pred_labels)\n",
    "    print \"%.03f\" %accuracy\n",
    "    print \"false +, false - : %d, %d\" %(false_pos.sum(), false_neg.sum())\n",
    "    \n",
    "    # get the request ids for the dev set and print ids for false pos or neg cases\n",
    "    np_ids = np.array(ids[3500:])\n",
    "    print np_ids[false_pos]\n",
    "    print pred_labels[false_pos]\n",
    "    print dev_labels[false_pos]\n",
    "\n",
    "# run gridsearch to find best k value\n",
    "def findK():\n",
    "    # range is from an iterative process to narrow down\n",
    "    params = {'n_neighbors': range(38,56,1)}\n",
    "    search = GridSearchCV(KNeighborsClassifier(), params)\n",
    "    search.fit(train_nums, train_labels)\n",
    "    \n",
    "    print \"the best parameter is k=%f\\n\" %search.best_params_['n_neighbors']\n",
    "    print \"summary of all params:\\n\", search.grid_scores_\n",
    "\n",
    "# train a simple BernoulliNB model and check accuracy\n",
    "# this uses the metadata\n",
    "def trainBern():\n",
    "    bern = BernoulliNB(alpha=0.0001)\n",
    "    bern.fit(train_nums, train_labels)\n",
    "    pred_labels = bern.predict(dev_nums)\n",
    "    \n",
    "    # display the accuracy\n",
    "    correct = (pred_labels == dev_labels)\n",
    "    accuracy = 1.0 * np.sum(correct) / len(dev_labels)\n",
    "    print \"accuracy for BernoulliNB is %.03f\" %accuracy\n",
    "\n",
    "# run gridsearch to find parameters for NB model\n",
    "def bernParams():\n",
    "    params = {'alpha': [0.0, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0]}\n",
    "    search = GridSearchCV(BernoulliNB(), params)\n",
    "    search.fit(train_nums, train_labels)\n",
    "    \n",
    "    print \"the best parameter is alpha=%f\\n\" %search.best_params_['alpha']\n",
    "    print \"summary of all params:\\n\", search.grid_scores_\n",
    "\n",
    "# train logistic regression using the metadata\n",
    "def trainNumLM():\n",
    "    lm = LogisticRegression()\n",
    "    lm.fit(train_nums, train_labels)\n",
    "    pred_labels = lm.predict(dev_nums)\n",
    "    \n",
    "    # print accuracy\n",
    "    correct = (pred_labels == dev_labels)\n",
    "    accuracy = 1.0 * np.sum(correct) / len(dev_labels)\n",
    "    print \"accuracy for LM is %.03f\" %accuracy\n",
    "    \n",
    "    # get false positive and negative cases and print data for the cases\n",
    "    false_pos = (pred_labels > dev_labels)\n",
    "    false_neg = (pred_labels < dev_labels)\n",
    "    print \"false +, false - : %d, %d\" %(false_pos.sum(), false_neg.sum())\n",
    "    np_ids = np.array(ids[3500:])\n",
    "    print np_ids[false_neg]\n",
    "    print lm.coef_[0]\n",
    "\n",
    "def trainSVM():\n",
    "    clf = SVC()\n",
    "    clf.fit(train_nums, train_labels)\n",
    "    pred_labels = clf.predict(dev_nums)\n",
    "    \n",
    "    # print accuracy\n",
    "    correct = (pred_labels == dev_labels)\n",
    "    accuracy = 1.0 * np.sum(correct) / len(dev_labels)\n",
    "    print \"accuracy for SVM is %.03f\" %accuracy\n",
    "    \n",
    "def trainRF():\n",
    "    clf = RandomForestClassifier(max_depth=5, n_estimators=10)\n",
    "    clf.fit(train_nums, train_labels)\n",
    "    pred_labels = clf.predict(dev_nums)\n",
    "    \n",
    "    # print accuracy\n",
    "    correct = (pred_labels == dev_labels)\n",
    "    accuracy = 1.0 * np.sum(correct) / len(dev_labels)\n",
    "    print \"accuracy for random forest is %.03f\" %accuracy\n",
    "    \n",
    "trainKNN(38)\n",
    "#findK()\n",
    "#trainBern()\n",
    "#bernParams()\n",
    "trainNumLM()\n",
    "#trainSVM()\n",
    "#trainRF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 1 components: 70.555556%\n",
      "Accuracy for 2 components: 70.185185%\n",
      "Accuracy for 3 components: 69.444444%\n",
      "Accuracy for 4 components: 68.703704%\n",
      "Accuracy for 5 components: 71.851852%\n",
      "Accuracy for 6 components: 63.148148%\n",
      "Accuracy for 7 components: 71.111111%\n",
      "Accuracy for 8 components: 72.962963%\n",
      "Accuracy for 9 components: 71.111111%\n",
      "Accuracy for 10 components: 68.148148%\n"
     ]
    }
   ],
   "source": [
    "# try using GMMs\n",
    "for gmm_comp in range(1,11):\n",
    "    gm1 = GMM(n_components=gmm_comp, covariance_type='full')\n",
    "    gm1.fit(train_nums[train_labels == 1, :])\n",
    "\n",
    "    gm2 = GMM(n_components=gmm_comp, covariance_type='full')\n",
    "    gm2.fit(train_nums[train_labels == 0, :])\n",
    "\n",
    "    # predict positive when first GMM returns higher score. compare this to test_labels\n",
    "    correct = (gm1.score(dev_nums) > gm2.score(dev_nums)) == dev_labels\n",
    "\n",
    "    # calculate accuracy\n",
    "    print \"Accuracy for %d components: %f%%\" %(gmm_comp, 100.0 * correct.sum() / correct.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
